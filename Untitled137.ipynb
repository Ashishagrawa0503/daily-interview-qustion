{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQYzXqRl4oCA33+IRkjuWm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishagrawa0503/daily-interview-qustion/blob/main/Untitled137.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EXPLAIN THE purpose of the euclidean distance metric in clustering tasks?\n",
        " -- eucliden distance in cluster for measuring similaruty between data points, helping to group similar instance together\n",
        " and guiding the structure and quality of cluster in algorithm like K-mean\n",
        " k-mean,hierarchical clustering DBSCAN are use eucliden distance\n",
        " Euclidean distance calculates the straight-line distance between two points in multi-dimensional space.\n",
        "Smaller distance → more similar the points are.\n",
        "\t•\tLarger distance → more dissimilar they are.\n",
        " Using Euclidean distance leads to spherical clusters(goal cluster bnata hai), because it treats all features equally\n",
        " and forms boundaries based on circular equidistance.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "JNG8LFGRm96d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#how do you handle situation where the data is not linearly separable?\n",
        "\"\"\"\n",
        "✅ 1. Use Non-Linear Models\n",
        "Switch to models that capture complex patterns:\n",
        "\t•\tDecision Trees / Random Forests\n",
        "→ Naturally handle non-linear decision boundaries.\n",
        "\t•\tSupport Vector Machines (SVM) with Kernels\n",
        "→ Use kernel trick (like RBF or polynomial kernels) to map data to a higher-dimensional space where it becomes\n",
        " linearly separable.\n",
        "\t•\tK-Nearest Neighbors (KNN)\n",
        "→ Non-parametric, works well even with irregular class boundaries.\n",
        "\t•\tNeural Networks (especially deep learning)\n",
        "→ Can learn highly non-linear decision surfaces through layers and activations.\n",
        "✅ 2. Feature Engineering / Transformation\n",
        "\t•\tPolynomial features: Add non-linear combinations of features (e.g., x_1^2, x_1x_2) to model curvature.\n",
        "\t•\tLog, exponential, or trigonometric transformations: Make patterns more distinguishable.\n",
        "✅ 3. Dimensionality Expansion\n",
        "\t•\tUse kernel methods or embedding techniques to map features into higher-dimensional spaces.\n",
        "\t•\tExample: SVM with RBF kernel maps inputs into infinite-dimensional space to find a separating hyperplane.\n",
        "✅ 4. Reduce Dimensionality (in some cases)\n",
        "\t•\tIf noise or irrelevant dimensions are obstructing separability, apply:\n",
        "\t•\tPCA (Principal Component Analysis)\n",
        "\t•\tt-SNE or UMAP (for visualization or clustering)\n",
        "✅ 5. Use Ensemble Methods\n",
        "\t•\tCombine weak models (e.g., using boosting) to capture complex boundaries.\n",
        "\t•\tExamples: Gradient Boosting, AdaBoost\n",
        "✅ 6. Accept Some Overlap and Use Probabilistic Models\n",
        "\t•\tModels like logistic regression or naive Bayes can still be used if you accept probabilistic outputs instead of hard boundaries\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rf5LFGVHWF_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "what is the purpose of the chi-square test in feture selection\n",
        "--use to determine indipendence between 2 catagorical variable and make it sutabite for classification task\n",
        "its a filter based feture selection model, it help eliminate feture that have no association with target,improving model\n",
        "performace and traning time ,high chi-square score  the more relevent feature.\n",
        "\"\"\"\n",
        "from sklearn.feature_selection import SelectKBest , chi2\n",
        "from skleran.datasets import load_iris\n",
        "import pandas as pd\n",
        "data = load_iris()\n",
        "x= pd.DataFrame(data.data,columns= data.feature_names)\n",
        "y= data.target\n",
        "selector= SelectorKBest(score_func=chi2 ,k=2)\n",
        "x_new= selector.fit_transform(x,y)\n"
      ],
      "metadata": {
        "id": "TFGMyD_JY6Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#can you explian the purpose of gradient descent algoritam in machine leraning?\n",
        "\"\"\"\n",
        "used in machine leraning to minimize a loss function and find the best parameter that result in the most\n",
        " accurate predictoion\n",
        " variants of gradient descent:-\n",
        " batch GD- usses the whole dataset per step( too slow)\n",
        " stochastic gd (sgd)- usses 1 sample per step(fast bust noisey update)\n",
        " mini-batch GD- usses small batch per step( balance of speed and accuracy)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "how do you handel sutiatio where the data is high- dimensional\n",
        "-pca and t-sne\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "how do you handle sutiotion where data is noise?\n",
        "-data cleaning- remove duplicates,correcrt missbalance data,\n",
        "-outfiler dection- z-score,IQR\n",
        "- smoothing techinque- gaussian filter,loess smoothing\n",
        "-feture selection- chi-squre, recursive feature elimination(RFE)\n",
        "-RObust method - loasso ridge tree based model\n",
        "\"\"\"\n",
        "#REF(recursive feature elimination)-\n",
        "\"\"\"\n",
        "its is feature selection techique  that help identify most important feature by removing least important ones on a model\n",
        "performance\n",
        "\"\"\"\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "x,y = load_iris(return_x_y= True)\n",
        "model = LogisticRegression()\n",
        "ref = RFE(model,n_feature_to_select=2)\n",
        "fit = ref.fit(x,y)\n",
        "print (fit.ranking_)\n",
        "\n",
        "#LOESS/ LOCALLY ESTIMATED SCATTERPLOT SMOOTHING\n",
        "\"\"\"\n",
        "USE TO SMOOTHING NOICE DATA ESPECIALLY IN TIME SERIES . , USE IN NON LINEAR DATA,\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Qj70kRDNcouu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Can you explain the bias-variance trade-off in the context of model complexity?\n",
        "\n",
        "The bias-variance trade-off is about finding the right balance between underfitting (high bias) and\n",
        " overfitting (high variance) by adjusting model complexity.\n",
        "Goal in ML:\n",
        "Build a model that generalizes well to unseen data — not just fits the training data perfectly.\n",
        "Trade-off Intuition\n",
        "\t•\tA simple model (e.g., linear regression on nonlinear data):\n",
        "\t•\tHas high bias → can’t capture complex patterns\n",
        "\t•\tHas low variance → changes little with new data\n",
        "\t•\tA complex model (e.g., deep neural network on small dataset):\n",
        "\t•\tHas low bias → can capture intricate relationships\n",
        "\t•\tHas high variance → learns noise → poor generalization\n",
        "Degree 1 (Linear model): High bias (underfits), low variance\n",
        "\t•\tDegree 2 (Quadratic): Low bias, low variance (optimal fit)\n",
        "\t•\tDegree 10 (High-degree polynomial): Low bias, high variance (overfits the noise)\n",
        "\"\"\"\n",
        "\n",
        "# how do you handle imbalanced dataset when buldinfg classification models -- over sampling under sampling ,smoothing\n",
        "#explian the purpose of reguarization - used to prevent overfitteng by add panalty term lamda (lasso and ridge for regularization)\n",
        "#how do you access the performance of a classification model apart from accuracy- recall,f1,precision\n",
        "\"\"\"\n",
        "explain the concept of feature selection and its importance in modle bullding\n",
        "-- rather then using all avalival feature you can select a subset that contribute the most too your model predictive power\n",
        "important for improve model accuracy, reduce over fitteng, simplifd moidel,speed up trainig,\n",
        " types of feture selection method\n",
        " 1.filter method- chi square,correaltional coefficient\n",
        " 2. RFE\n",
        " 3.lasso  tree base model\n",
        " \"\"\"\n",
        " # correlational coefficient-\n",
        " \"\"\"\n",
        " the correlational cofficient measures the strength and direction on relatonship between two variables\n",
        " is a value between -1 to +1\n",
        "  if 1 - perferct positive corelation\n",
        "  if 0 -no linear corelation\n",
        "  if -1 -perfect negative corelation\n",
        "\n",
        " why we use corelation for feature selection -high correlation with target - then feature is informative\n",
        "  high correlation with other feature - lead to multicollinearity\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "ebfTkFdCtNaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA preprocerssing\n",
        "\"\"\"\n",
        " Data preprocessing is a crucial step in the machine learning pipeline. Without it, even the most powerful\n",
        "  algorithms can produce poor results.Purpose of Data Preprocessing\n",
        "Data preprocessing is the process of cleaning and transforming raw data into a format that is suitable for\n",
        "machine learning models.\n",
        "📌 Why it’s important:\n",
        "\t•\tEnsures data quality (removes noise, missing values, errors)\n",
        "\t•\tHelps models converge faster and perform better\n",
        "\t•\tMakes data consistent and comparable\n",
        "\t•\tReduces bias, variance, and potential overfitting\n",
        "  🛠️ Common Data Preprocessing Techniques\n",
        "\n",
        "1. Handling Missing Values\n",
        "\t•\tRemove rows or columns with too many missing values\n",
        "\t•\tImpute missing data:\n",
        "\t•\tMean/Median imputation (for numeric)\n",
        "\t•\tMode imputation (for categorical)\n",
        "\t•\tKNN imputation or regression-based imputation\n",
        " 2. Encoding Categorical Variables\n",
        "\t•\tConvert text labels into numbers\n",
        "\t•\tLabel Encoding\n",
        "\t•\tOne-Hot Encoding\n",
        " 3. Feature Scaling\n",
        "\t•\tBring features to the same scale:\n",
        "\t•\tStandardization (mean = 0, std = 1)\n",
        "\t•\tMin-Max Scaling (values between 0 and 1)\n",
        "\t•\tRobust Scaling (uses median and IQR)\n",
        " 4. Outlier Detection & Removal\n",
        "\t•\tRemove or cap extreme values\n",
        "\t•\tTechniques:\n",
        "\t•\tZ-score method\n",
        "\t•\tIQR method\n",
        "\t•\tIsolation Forest or DBSCAN (advanced)\n",
        "5. Noise Reduction\n",
        "\t•\tUse smoothing techniques, filtering, or dimensionality reduction (e.g., PCA)\n",
        "6. Text Preprocessing (for NLP)\n",
        "\t•\tTokenization\n",
        "\t•\tLowercasing\n",
        "\t•\tStop-word removal\n",
        "\t•\tLemmatization or stemming\n",
        "\t•\tVectorization (TF-IDF, Word2Vec)\n",
        "7. Feature Extraction & Selection\n",
        "\t•\tDerive new features or select the most relevant ones\n",
        "\t•\tTechniques:\n",
        "\t•\tPCA\n",
        "\t•\tRecursive Feature Elimination (RFE)\n",
        "\t•\tMutual Information, Chi-Square\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tarRPB8E3d7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explain the concept of 'hyperparmeter tnuing' in machine leraning algorithm\n",
        "\"\"\"\n",
        "hrperparameter tuning is the process of finding the optimal value for the parameter that control\n",
        " the behabiour of a machine learning algorithm- befor traning begain.\n",
        " parametre -learned from the data during traning\n",
        " hyperparameter- set befor traning cintros the traning process ex-linear regression,decision tree,random forest,knn,\n",
        " svm, neural network\n",
        " why hyperparameter tunning important\n",
        " -improves model performance, reduce overfitting/underfitting\n",
        " common hyperparameter tunning techniques-\n",
        " 1.Grid search- all possible combinations from a predefined set of values\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(grid.best_params_)\n",
        "\n",
        " 2.Randomized search-Try a random sample of combinations. More efficient than Grid Search.\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),\n",
        "    'max_depth': randint(5, 20)\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist, n_iter=10, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(random_search.best_params_)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mbq4aiDm5FiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "what is the purpose of 'ANOVA' (Analysis of varience) in statical analysis and when is it used\n",
        "used to compare the mean of two or more  groups to detarmine whether at least one group mean is\n",
        "significantly different from other group.\n",
        "types of anova\n",
        "1.one-wat-anova=comparing means across 1 catagorica variable\n",
        "2.two-way-anova=comparing means across 2 catagorica variable\n",
        "3.manova= multiple dependent variables\n",
        "4. repeted measures= time seris\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DWXT_PR3a1q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#can you explain the purpose of cosine similarity in similarity measurements?\n",
        "\"\"\"\n",
        "cosine similirty is a matrix used to measure how similar two vector are, based on the angle between them.itis usful\n",
        "in high dimensional such as text data . evaluate similarity between two data points. common in nlp and recommendation\n",
        "system\n",
        "\"\"\"\n",
        "from sklaern.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "docs = ['machine learning is fun','machine learning is intresting','apple are testy']\n",
        "vector = TfidfVectorizer()\n",
        "matrix = vector.fit_transform(docs)\n",
        "cos_sim = cosine_similarity(matrix)\n",
        "print(cos_sim)\n",
        "\n"
      ],
      "metadata": {
        "id": "UGPe1uVndr9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EvYBklxcgUbI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}