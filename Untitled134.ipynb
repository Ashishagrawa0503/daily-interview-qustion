{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQuU8nAto6Jpf+fMHGa9eS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishagrawa0503/daily-interview-qustion/blob/main/Untitled134.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXZmrH50Yi5R",
        "outputId": "6466ca66-4b9c-4dca-85a9-600bc1e31c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how many time multiply 5 to get 525. That's how many times you have to multiply 5 to get 1.\n",
            "\n",
            "If you are going to get 5 times your life, multiply 5 times. That's how many times you have to multiply 5 to get 5.\n",
            "\n",
            "If you are going to get 5 times your life, multiply 5 times. That's how many times you have to multiply 5 to get 5.\n",
            "\n",
            "If you are going to get 5 times your life, multiply 5 times. That's how many times you have to multiply 5 to get 5.\n",
            "\n",
            "If you are going to get 5 times your life, multiply 5 times. That's how many times you have to multiply 5 to get 5.\n",
            "\n",
            "If you are going to get 5 times your life, multiply 5 times. That's how many times you have to multiply 5 to get 5.\n",
            "\n",
            "If you are going to get 5 times your life, multiply 5 times. That's how many times you have to multiply 5 to get 5.\n",
            "\n",
            "If you are going to get 5 times your life, multiply 5 times. That's how many times you have to multiply 5 to get 5.\n",
            "\n",
            "If you are going to get 5 times your life, multiply 5 times. That's how many times you have\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "1.CoT(chai of thoughts)= chain of thought is a powerfull techique used in large language models to improve reasoning by\n",
        "encouraging the model to think step by step insted of jumping to conclusion\n",
        "\n",
        "ex-Q: If a train travels 60 km in 1 hour, how far does it go in 3 hours?\n",
        "=without cot its -  180\n",
        "A:with cot -  The train travels 60 km per hour. In 3 hours, it would travel 60 × 3 = 180 km. So, the answer is 180.\n",
        "\"\"\"\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')#gpt2 model is CoT MODEL\n",
        "prompt = \"how many time multiply 5 to get 525\"\n",
        "response = generator(prompt, max_length=200, do_sample=True, top_p=0.9)\n",
        "print(response[0][\"generated_text\"])#generated_text help convert string to text the model produce"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "2. what is the different between dicriminative and generative ai\n",
        "dicriminative= focuses on pedicting or classify data based on existing data.ex-stock price detection spam detection ,\n",
        "image classification\n",
        "generative ai = focuses on generating new data samples that resemble (same as require data) the training data.\n",
        "ex-image generation\n",
        "\"\"\"\n",
        "#generative model\n",
        "from transformers import pipeline\n",
        "generator= pipeline(\"text_generation\",model=\"gpt2\")\n",
        "prompt=\"ones uppon a time in a distant galaxy\"\n",
        "output = generator(prompt,max_length=50,max_new_tokens=50)\n",
        "print (output[0],[\"generated_text\"])\n",
        "\n",
        "#discrimantive ai\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "texts=[\"buy cheep pils now!\",\"important meeting at 10\",\"win a millin dollars\",\"lunch with a client\"]\n",
        "labels=[1,0,1,0]\n",
        "vector= CountVectorizer()\n",
        "x= vector.fit_transform(texts)\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x,labels)\n",
        "#predict\n",
        "new_email= [\"free varification offer!\"]\n",
        "x_new= vector.transform(new_email)\n",
        "pred= logistic.predict(x_new)\n",
        "print (\"spam\" if pred[0]==1 else \"not spam\")"
      ],
      "metadata": {
        "id": "JWT3HtX5ZYGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "3. what are some common challenges associated with using llm\n",
        "-they require substantial(accha -khasa) computing power and money ,making both training and deployment.\n",
        "-it can leran and reproduce biases from their training data, potentially(sambhawtah) leading to biase and unfair outputs.\n",
        "-trainig on large dataset can raise conserns about data privacy and sequirety\n",
        "-it can be very costly\n",
        "\n",
        "4- what is large language model?\n",
        "-a llm is an ai system trained on vest amount of text data to unerstand,generates,and predict humen like text or language.\n",
        " it learns patterns ,context, and relationship in data to produce relevent coherent responses\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "44D8n61jlJmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "5.can you explain the concept of attention mechanisum in transformer model?\n",
        "-at a high level attention mechanisum allows the model to focus on differnt parts of the input sequence when making prediction.\n",
        " insted of treating every word or token equally ,model learn to relevent words to contibute most to the current prediction\n",
        " ex-For example, in a sentence like \"The dog chased the ball because it was fast,\" the word \"it\" could refer to either the\n",
        " dog or the ball. The attention mechanism helps the model figure out that \"it\" is likely referring to \"the ball\" based on\n",
        " the context.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "query = torch.randn(1,3,4)\n",
        "key = query.clone()\n",
        "value= query.clone()\n",
        "#formula making of attention mechanesum q,k,v= softmax(qk.transpose/d_k)*v\n",
        "scores = torch.mutmul(query,key.transpose(-2,-1))#transpose ek ek cloums ko row mai change ke diya\n",
        "d_k= query.size(-1)# number or cloumns (diminsions or embeddng size)\n",
        "scores = scores/d_k**0.5\n",
        "attention_weights = F.softmax(scores,dim=-1)\n",
        "output = torch.matmul(attention_weights,value)\n",
        "print ( attention_weights)\n",
        "print( output)"
      ],
      "metadata": {
        "id": "-wRltB-hnoA0",
        "outputId": "f6040de6-a7e0-49e8-a229-ae1d6d0b4cf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.8084, -0.8631,  2.2877,  0.1642],\n",
              "         [-0.1440,  2.2761, -3.1344, -0.0285],\n",
              "         [-2.3037,  0.8000, -1.3744, -0.2336]]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "6.what is context window?\n",
        "- the context window is the maximum number ot tokens (word subword charecters ) a language model can \"see\" or process at one\n",
        "at time. a large context window means the model can incorporate more surrounding information , which enhancces its understanding\n",
        "and ability to generat text\n",
        "\n",
        "7.what are the main difference between llm and traditional statstical language models ?\n",
        "- 1.architecture- llm are based on self attention,which chapture long range dependencies,unlike traditional model like n-grams knn ect\n",
        "-2.scale-llm has billions of parameters and train on massive datasets for better generalization, traditional model are smaller\n",
        "and task-spacific\n",
        "-3.traning- llm undergo unsuperwise and pre-tarining and fine-tune models,traditional models rely or superwise learinig with\n",
        "labeled dataset\n",
        "-4.-flazxibility-LLMs can tackle multiple NLP tasks with little fine-tuning, while traditional models are designed for specific\n",
        " tasks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WOaCbMSluxIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "8. what is the difference between encoder and decoder.\n",
        "In the transformer architecture used in large language models, the encoder and decoder serve different purposes.\n",
        "The encoder processes the input data and transforms it into a set of abstract representations. The decoder then takes\n",
        "these representations and generates the output, using both the information from the encoder and the previously generated\n",
        "elements in the sequence. Essentially, the encoder is responsible for understanding the input, while the decoder focuses\n",
        "on producing the final output.\n",
        "\n",
        "9.You're working on an LLM, and it starts generating offensive or factually incorrect outputs. How would you diagnose and address this issue?\n",
        "Ans - If an LLM produces offensive or inaccurate outputs, I would first analyze the patterns, check input prompts,\n",
        "and assess if the issue stems from biases or gaps in the training data. I'd review the preprocessing pipeline for errors\n",
        " or biases and examine the dataset for imbalances.\n",
        "Structural Issues Analyze Patterns Check Hyperparameters\n",
        "* Model Architecturn\n",
        "Review,Preprocessing,Adversarial,Training,Data Biases\n",
        "Next, l'd evaluate the model's architecture, hyperparameters, and fine-tuning to identify any structural issues.\n",
        " Solutions could include adversarial training, debiasing, data augmentation, or retraining with a more balanced dataset.\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "10. Explain the concept of \"few-shot learning\" in LLMs and its advantages.\n",
        "Ans - Few-shot learning in LLMs is the ability of the model to understand and tackle new tasks with just a few examples.\n",
        " This is made possible by the model's extensive pre-training, which allows it to generalize from limited data.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "11.what is vanishing gradient problem , and how does the transformer architecture address it?\n",
        "- when training with backpropagation the gradients(use to update wights) can become very small,so earlier learye learn\n",
        " very slow or not at all\n",
        " so transformer avoid the vanishing gradient problem by\n",
        " -1.transformer dont process sequence step-by-step ,insted they look at whole sequence in paraller using self-attention\n",
        " -2.layer normalization reduce the risk of vanishing graduent problem\n",
        " -3. residual connections = skip connection between layers allow gradient to flow directly to earlier layers\n",
        "\"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Dxq-oi-_00tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "12.adaptive softmax speed up llm?\n",
        "-yes especially when delling with large vocabularies\n",
        "in normal language models:-the softmax layer computs scores fro every word in vocabulary and this computional expansive\n",
        " for vocab size like 50000k words. adaptive softmax splitting the vocabulary into cluster : head:- frequent word(\"the,\"and\")\n",
        " tail:-rare word ,advantages - speed up  traning ,reduce memory usage, used in model like gpt2,fairseq,deepspeed\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "13.what is zero short learning and how does is apply to llm?\n",
        "-ZSL is ability to a model to perform a task it has never see any labeled examples just by undesanding contex(tell the model\n",
        "what to do ,and it tries to do it , with zero example )\n",
        "llm models like gpt2, gpt3 ect are pre trained on vest amount of data including task like summarixation translation qustion\n",
        " ansering because of this perform zero short learning by using nlp prompts\n",
        "\"\"\"\n",
        "#example of zero shot learning\n",
        "from transformers import pipeline\n",
        "classifer = pipeline (\"zero-shot-calssification\", model=\"facebook/bart-large-mnli\")\n",
        "sentence = \"this book is relly good and entertaning\"\n",
        "labels =[\"positive\",\"negaive\",\"neutral\"]\n",
        "result = classifer(sentence,candidate_labels=labels)\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "id": "6EpYkYU29LEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "14.HOW does the mixture of experts (moe) technique improve llm stability\n",
        "-MoE activates only a small subset of a large model for each input, instead of using the whole model every time.\n",
        "Imagine a huge team of experts, and for each input, only 2 or 3 experts are consulted — not the whole team.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "I2thbshgD6ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AezXDvUt3zYj"
      }
    }
  ]
}