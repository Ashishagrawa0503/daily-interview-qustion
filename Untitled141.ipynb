{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7BE7sg5CFmr+6o20hqUKs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishagrawa0503/daily-interview-qustion/blob/main/Untitled141.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7QKGyTQvd3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "softmax activation function\n",
        "--Softmax function का उपयोग deep learning में तब किया जाता है जब हमें multi-class classification (एक से अधिक class में से एक चुननी हो)\n",
        " करनी होती है।\n",
        "Softmax एक activation function है जो logits (raw scores) को probabilities में बदलता है। ये इस तरह काम करता है कि:\n",
        "\t•\tहर class के लिए एक probability देता है (0 और 1 के बीच)\n",
        "\t•\tसभी probabilities का योग (sum) = 1 होता है\n",
        "मान लो, एक model का output है:\n",
        "[2.0, 1.0, 0.1] (तीन classes के लिए)\n",
        "Softmax उसे probabilities में बदल देगा जैसे:\n",
        "[0.65, 0.24, 0.11]\n",
        "(Total = 1)\n",
        "अब सबसे ज़्यादा probability class 0 की है, तो prediction = class 0\n",
        "अगर आप CrossEntropyLoss PyTorch में use कर रहे हो, तो softmax अलग से मत लगाओ — वो अंदर ही लगा होता है।\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "log= torch.tensor([1.0,2.0,3.0])\n",
        "print(F.softmax(log,dim=0))\n",
        "\n",
        "sigmoid activation function\n",
        "--Sigmoid activation function का उपयोग तब किया जाता है जब deep learning model का काम binary classification होता है यानी:\n",
        "सिर्फ दो classes में से एक को predict करना होता है — जैसे हाँ या ना, 0 या 1, positive या negative।\n",
        "Output → Probability of class 1\n",
        "\t•\tअगर output > 0.5 → Class 1\n",
        "\t•\tअगर output < 0.5 → Class 0\n",
        "अगर आप BCEWithLogitsLoss use कर रहे हैं, तो sigmoid manually ना लगाएँ (वो internally handle करता है)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ytNUp7sQzhr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "irreducible error\n",
        "--jab data collect kiya jata hai us time he data main noise yaa data adhura rahega jiske karan output main error aayega jiske\n",
        "irreducible error hai.\n",
        "\n",
        "k-fold cross validation --\n",
        "--K-Fold Cross Validation एक technique है जिसका उपयोग हम machine learning में करते हैं ताकि model को ज्यादा अच्छी तरह से train और validate\n",
        " कर सकें और उसे overfitting या underfitting से बचा सकें।\n",
        "\n",
        " dropout_regularization--\n",
        " --Dropout एक regularization technique है जो deep learning में overfitting को रोकने के लिए use की जाती है।\n",
        "❝ Dropout का मतलब है कि training के दौरान neural network के कुछ neurons को randomly “off” कर देना। ❞\n",
        "इससे model हर बार थोड़ा अलग path से सीखता है → और overfitting से बचता है।\n",
        "\n",
        "adgrad(adaptive gradient)\n",
        "--Adagrad (Adaptive Gradient) एक optimization algorithm है जो deep learning में gradient descent को बेहतर बनाने के लिए use होती है।\n",
        "यह learning rate को हर parameter के लिए अलग-अलग और self-adjusting बनाता है।\n",
        "Adagrad हर parameter के लिए अलग learning rate रखता है, और उसे past gradients के हिसाब से adjust करता है।\n",
        "Use Cases-NLP, Recommender Systems, Sparse inputs\n",
        "Main drawback:Gradient accumulation के कारण learning rate बहुत जल्दी zero के पास चला जाता है, जिससे training रुक सकती है।\n",
        "1.SGD-Fixed learning rate\n",
        "2.Adagrad-Adaptive rate, good for sparse data\n",
        "3.RMSProp-Fixes Adagrad’s decaying rate issue(learning rate kam kr deta hai jisse traning ruk jate hai)\n",
        "  root mean square prop ye adaptive gradient ka improved version hai\n",
        "4.Adam-Combines RMSProp + momentum (best default)\n",
        "momentum in optimizer - Momentum एक technique है जिसे gradient descent को faster और more stable बनाने के लिए use किया जाता है।\n",
        "Imagine a ball rolling down a hill:\n",
        "\t•\tअगर path smooth है → ball speed पकड़ लेता है (momentum बढ़ता है)\n",
        "\t•\tअगर path में bump है → ball थोड़ा हिचकता है पर रुकता नहीं\n",
        "इसी तरह, momentum oscillations को कम करता है और model को fast converge करने में मदद करता है।\n",
        "Sgd-यह हर बार सिर्फ current gradient के direction में चलता ह\n",
        "Without momentum: learning jumps back and forth(aange peche hota rhata hai)\n",
        "•\tWith momentum: smoother path, less zig-zag, faster convergence\n",
        "Optimizer   Momentum    Adaptive LR  Comment\n",
        "SGD          ❌         ❌           Basic optimizer\n",
        "Momentum     ✅            ❌           Smoother than SGD\n",
        "RMSProp      ❌             ✅           Solves Adagrad’s decay\n",
        "Adam         ✅            ✅            Best of both worlds\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "v_Xn_WV33eWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "convolutional operation/layer -\n",
        "--Convolution एक ऐसा mathematical process है जिसमें हम एक छोटा matrix (जिसे filter/kernel कहते हैं) को input image पर slide करते हैं और features (जैसे edges, textures) को detect करते हैं।\n",
        "Convolution operation से हमें एक smaller output matrix (feature map) मिलेगा।\n",
        "कहां use होता है-CNNs, Image Classification, Object Detection\n",
        "Feature Detection Lines, edges, textures पकड़ता है\n",
        "Parameter Sharing हर filter image में same weight use करता है\n",
        "Dimensionality Reduction Image size छोटा होता है (with pooling)\n",
        "Image Understanding Object detection, classification आदि में use\n",
        "CNN का पूरा architecture हैं (convolution → pooling → flatten → dense → output)\n",
        "pooling- Pooling input feature map को छोटे हिस्सों में divide करता है और हर हिस्से का summary value निकालता है\n",
        "1.Max Pooling--हर region से सबसे बड़ा (max) value लेता है\n",
        "2.Average Pooling--हर region का average निकालता ह\n",
        "3.Global Average Pooling--Entire feature map को एक value में reduce करता है (classification के पहले)\n",
        "Advantage Dimensionality Reduction,Feature map छोटा होता है → computation तेज होता है overfitting kam karta hai\n",
        "flatten -- यह multi-dimensional (2D/3D) feature map को एक लंबी 1D vector में बदल देता है — ताकि उसे fully connected\n",
        " (dense) layer में भेजा जा सके। Flatten” का मतलब है कि 2D या 3D matrix को सिर्फ एक single line (vector) में बदल देना।\n",
        "dense --har neuron dusre layer ke har neuron se conneceted rhata hai fully connected layer\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "3HwEQ9bVKLVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "self organizing map(som) kohonen maps--\n",
        "Kohonen Map, जिसे हम Self-Organizing Map (SOM) भी कहते हैं, एक unsupervised neural network है, जो high-dimensional\n",
        "data को low-dimensional (जैसे 2D grid) में map करता है — while preserving similarity.\n",
        "Kohonen Map एक तरह का “neural clustering” है, जो data को खुद से organize करता है — बिना किसी label के।\n",
        "Dimensionality Reduction\n",
        "PCA से अलग – nonlinear structure भी समझता है\n",
        "Use in -Bioinformatics, Data Visualization(pca,tshine,umap ye shabi data visualisation main upyog hote hai )\n",
        "Geography, Market Segmentation\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7GrjNMlbe_69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbEo_wP2izD6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}