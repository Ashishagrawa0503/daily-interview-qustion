{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxq91j3koIX+KYYgpvYROu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishagrawa0503/daily-interview-qustion/blob/main/Untitled135.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0B5ag11LIoS"
      },
      "outputs": [],
      "source": [
        "\"\"\"1.how does parameter-efficient-fine-tuning(PEFT) PREVENT catastrophic forgetting in LLM?\n",
        "-when you fine tuning large language model on a new task or dataset,it may forget previously learned knowladge-this is\n",
        "called CATASTROPHIC forgating for example you train a model for madical q/a but its forgeting who to math or resining\n",
        "so PEFT - method adapt llm to new task by only updating small portion of the model parameters insted updating all weights\n",
        "common peft techiques= 1.LORA,adapter layer,prefix tunning, bitfit\n",
        "--what is LORA and QLORA\n",
        "--LORA AND QLORA  are tecnique designed to optimize the fine-tune of LLM ,focusing on reducing memory usage and enhancing\n",
        "  efficiency without compromising permormance in NLP tasks\n",
        "LORA -\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. What are different types of Foundation Models?\n",
        "-Foundation models are large-scale Al models trained on vast amounts of unlabeled data using unsupervised methods.\n",
        " Common Types of Foundation Models-\n",
        "1. Language Models -\n",
        "Tasks: Machine translation, text summarization, question answering\n",
        "Examples: BERT, GPT-3\n",
        "2. Computer Vision Models -\n",
        "Tasks: Image classification, object detection, image segmentation\n",
        "Examples: ResNet, VGGNet\n",
        "3. Generative Models -\n",
        "Tasks: Creative writing, image generation, music composition\n",
        "Examples: DALL-E, Imagen\n",
        "4. Multimodal Models -\n",
        "Tasks: Image captioning, visual question answering Examples: PaLM, LaMDA\n",
        "\n",
        "3.what is tha chain rule in calculus and how does it apply to gradient descent in deep learning\n",
        "ans- in deep learning the chain rule is used in backpropagation to compute gradient of the loss function with respect to each parameter\n",
        "layer by layer. this allow gradient descent to update weights efficiently(kushlta se )\n",
        "\n",
        "4.what is KL(kullback leibler) divergence used in evaluating llm outputs ?\n",
        "ans- KL divergence measures how one probality distribution is different from another-\n",
        "p- the true and target distribution\n",
        "q- the model predicited distribution\n",
        "-the divergence 0 when p=q\n",
        "\n",
        "5.what is beam search and how does it different from greedy decoding?\n",
        "-greedy decoding is the simplest way to generate text froom a language model.at each step it picks the single most likhly\n",
        " next word. speed fast ,low memory to store sequence, not give better sentance\n",
        "ex-input-\"the dog\"\n",
        "pick the next word: \"is\"\n",
        "then \"sleeping\"\n",
        "output -\"the dog is sleeping\"\n",
        "-beam search : insted of picking only one word it keep the N TOP possible sequence at each step and explore. speed slow ,\n",
        "generate batter words ,higher memory to store multiple sequence\n",
        "ex- input:-\"the dog\"\n",
        "tries 2 top continuation\n",
        "\"is cute\"\n",
        "\"was barking\"\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 fix\n",
        "\n",
        "input_text = \"The future of AI is\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Greedy decoding\n",
        "greedy_output = model.generate(**inputs, max_new_tokens=10)\n",
        "print(\"Greedy:\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Beam Search decoding (beam width = 3)\n",
        "beam_output = model.generate(**inputs, max_new_tokens=10, num_beams=3, early_stopping=True)\n",
        "print(\"Beam Search:\", tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
        "\n",
        "\"\"\"\n",
        "6. explain the cncept of temprature in llm text generation\n",
        "--temprature controls the randomness or creativity of the text generated by a language model.it is a single number\n",
        "usualluy between 0.0to1.0 .low tramputer makes model more confident and focuse on high probility of word ,but higher\n",
        "temprature make model more random and creative about predicted word\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokinizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "prompt=\" the book is\"\n",
        "inputs = tokinizer(prompt,return_tensors=\"pt\")\n",
        "low_output= model.generate(inputs[\"input_ids\"],max_new_tokens=20,temperature=0.3)\n",
        "print (tokinizer.decode(low_output[0],skip_special_tokens=True))\n",
        "high_output= model.generate(inputs[\"input_ids\"],max_new_tokens=20,temperature=2.0)\n",
        "print (tokinizer.decode(high_output[0],skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "7.what is masked langage modeling and how does it contribute model pretraning\n",
        "--MLM is a training strategy where some words in a sentence are hidden (masked), and the model is trained to\n",
        " predict the missing words.üßæ Example:\n",
        "Original sentence:\n",
        "‚ÄúThe cat sat on the mat.‚Äù\n",
        "Masked version (input to the model):\n",
        "‚ÄúThe cat sat on the <mask>.‚Äù\n",
        "üëâ The model has to predict: \"mat\"\n",
        "‚∏ªüéØ Why Use MLM?\n",
        "\t‚Ä¢\tIt forces the model to understand the context around a missing word.\n",
        "\t‚Ä¢\tThe model learns grammar, meaning, and relationships between words.\n",
        "\t‚Ä¢\tThis helps build strong language understanding, which is crucial for downstream tasks like classification, Q&A, etc.\n",
        "\"\"\"\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "# Example sentence with a [MASK] token\n",
        "text = \"The capital of Germany is [MASK].\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "# Get logits and predict masked token\n",
        "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "#inputs.input_ids ye ek tensor hai jo sentance ke id (1,212,234,456,456) ko stare krke rakhta hai\n",
        "#inpus.input_ids==tokenizer.mask_token_id comapir each id with mask id (ex there is mask id is 456) and return the\n",
        "#boolen tensor showing where is mask (false,false,false,false,true)\n",
        "#[0] access first row of the tensor\n",
        "#.nonzero(as_tuple=True) find the index where is value True and as_tuple return flat list of position\n",
        "predicted_token_id = outputs.logits[0, mask_token_index].argmax(dim=-1)\n",
        "#outputs.logits raw prediction for every token,[0,mask_token_index] for focuse on mask prediction,argmax(dim=-1)pick most likly word highest score\n",
        "predicted_word = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(\"Prediction:\", predicted_word)\n",
        "\n",
        "\"\"\"\n",
        "8.what are sequence-to-sequence models?\n",
        "-- seq2seq models are a class of neural architech to designed to transform one seq into another. they are back bone of machine\n",
        "translation, text summerization,conversational agents\n",
        "architecture(encoder-compress the input into a fix-length context (vector) ,decoder - taks that context and generatw output)\n",
        "attention-mechanisum - allows to decoder to focus on different parts of the encoder output\n",
        "\"\"\"\n",
        "from transformers import T5Tokenizer,T5ForConditionalGeneration\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "text = \"translate English to French : i love learning\"\n",
        "inputs = tokenizer(text,return_tensor=\"pt\")\n",
        "outputs = model.generate(**inputs,max_new_tokens=50,num_beams=4,early_stoping=True)\n",
        "translate=tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
        "print (translate)\n",
        "#T5ForConditionalGeneration is a Seq2Seq model with a T5-style encoder and decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "9.how do autoregressive models differ from masked models in llm training\n",
        "-- auto regrassive model such as GPT generates text one token at time.with each token predicted based on previously generated\n",
        "token this aproch idel for text generation  where mask model suited for classification or understading tasks such as question\n",
        "answering\n",
        "\n",
        "10.what is next sentence predicition(NSP) and why is it important in llm training\n",
        "NSP IS KEY techinique used in llm like BERT.NSP help a model understanding the relationship between sentace 1 and\n",
        "sentance 2. it is important for qustin answering task\n",
        "50% (IS NEXT) of time sentance 2 is follows sentance 1\n",
        "50% (NOT NEXT) of time sentance 2 is random for  sentance 1\n",
        "\n",
        "10.Explain the difference between top-k sampling and nucleus (top-p) sampling in LLMs.\n",
        "Ans - Top-k sampling restricts the model's choices to the top k most probable tokens at each step, introducing controlled\n",
        " randomness.\n",
        "For example, setting k=10 means the model will only consider the 10 most likely tokens. Nucleus sampling,\n",
        " or top-p sampling, takes a more dynamic approach by selecting tokens whose cumulative probability exceeds a threshold\n",
        "  p (e.g., o.g). This allows for flexible candidate sets based on context, promoting both diversity and coherence\n",
        "  in generated text.\n",
        "\n",
        "11.What is model distillation, and how is it applied to LLMs?\n",
        "Ans - Model distillation is a technique where a smaller, simpler model (student) is trained to replicate the behavior of a\n",
        "arger, more complex model (teacher). In the context of LLMs, the student model learns from the teacher's soft predictions\n",
        "rather than hard labels, capturing nuanced knowledge. This approach reduces computational requirements and memory\n",
        " usage while maintaining similar performance, making it ideal for deploying LLMs on resource-constrained devices.\n",
        "\n",
        "\n",
        "Q12. How do LLMs handle out-of-vocabulary (OOV) words? what is BPE and WordPiece\n",
        "Ans - Out-of-vocabulary words refer to words that the model did not encounter during training. LMs address this\n",
        " issue through subword tokenization techniques like Byte-Pair Encoding (BPE) and WordPiece. These methods break down\n",
        "  OOV words into smaller, known subword units. For example, the word \"unhappiness\" might be tokenized as\n",
        "   \"un,\" \"happi,\" and \"ness.\" This allows the model to understand and generate words it has never seen before\n",
        "    by leveraging these subword components\n",
        "\n",
        "13. how does the transformer archi the challenges faced by traditional seq2seq model\n",
        "-Traditional Sequence-to-Sequence Models (like RNNs, LSTMs) ‚Äì Challenges\n",
        "\t1.\tSlow processing\n",
        "‚Üí They handle words one at a time, so they can‚Äôt be parallelized well.\n",
        "\t2.\tHard to remember long sentences\n",
        "‚Üí Important info from earlier in the sentence may get lost by the time the model reaches the end.\n",
        "\t3.\tFixed-length bottleneck\n",
        "‚Üí They squash the whole input sentence into a single vector before decoding. That‚Äôs like compressing a book into one sentence ‚Äî it loses detail.\n",
        "‚ö° Transformer ‚Äì How It Solves These\n",
        "\t1.\tParallel processing\n",
        "‚úÖ Instead of going word-by-word, Transformers look at all words at once. Much faster and more efficient.\n",
        "\t2.\tSelf-Attention mechanism\n",
        "‚úÖ The model can focus on important words anywhere in the sentence, no matter how far apart they are.\n",
        "‚Üí For example: it knows that in ‚ÄúThe cat that chased the mouse was black‚Äù, ‚Äúcat‚Äù connects to ‚Äúwas‚Äù, not ‚Äúmouse‚Äù.\n",
        "\t3.\tNo fixed-length bottleneck\n",
        "‚úÖ Each word gets its own vector and context, instead of cramming everything into one.\n",
        "‚Üí This gives more room to remember details.\n",
        "\t4.\tBetter scalability\n",
        "‚úÖ Easier to train bigger models and on bigger datasets, which leads to better performance.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hgOvhTSCu-Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "14.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yAG0whtM4EEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"the \"AutoModelForCausalLM\" class from the transformers library is used for loading and using large language models\n",
        " (LLMs) specifically designed for causal language modeling.\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "NCbBsyW6SbPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAbJuE7FdK27"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}