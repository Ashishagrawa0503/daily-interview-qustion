{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBoSprHlXkj5sC5KR1WP+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishagrawa0503/daily-interview-qustion/blob/main/Untitled136.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElTgt_qK5Q9N"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. explain the differance between superwise and unsuperwise learning\n",
        "--superwise learning use labeled datd for traning while unsuper wise learning used unlabeled data for traning .and find the\n",
        "hidden patterns and relationship in the data\n",
        "\n",
        "2.what is cross validation and why its important\n",
        "--cross validation is a technique used in machine learning to test how well your model will perform on unseen data\n",
        "insted of train your all data you can split the data into multiple parts train and test ex- k-fold cross validation\n",
        "\n",
        "3. can you explain the steps involed in data preprocessing process?\n",
        "-data preprocessing includes data cleaning,handiling missing value,naormalization,standrization\n",
        "\n",
        "4.what are some common algoritm used in machine learning\n",
        "-- linear regression,logistic regression, decision tree , svm, neral network\n",
        "\n",
        "5.how do you handle missimg data in a dataset?\n",
        "-- missing data can be handling by either removing the rows with missing values,imputation the missig value with mean,median,mode\n",
        "or advance impution tecnique like knn imputer\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "data= {\n",
        "    'A':[1,2,np.nan,4],\n",
        "    'B':[5,np.nan,7,8],\n",
        "    'C':[9,10,11,np.nan]\n",
        "}\n",
        "df=pd.DataFrame(data)\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "df_imputed = imputer.fit_transform(df)\n",
        "print(df_imputed)\n",
        "\n",
        "\"\"\"\n",
        "6.what is the pourpose of k-mean clustring algoritm\n",
        "-- k-means algoritm is used for partition the data set into k cluster\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs# use to make nakli dataset for clustring\n",
        "x= make_blobs(n_samples=300,n_features=2,centers=3,random_state=42)\n",
        "kmeans=KMeans(n_clusters=3,random_state=42)\n",
        "kmeans.fit(x[0])\n",
        "print(kmeans.labels_)\n",
        "print(kmeans.cluster_centers)\n",
        "\n",
        "\"\"\"\n",
        "7.explain the term 'bias '  in context of machine learning models/\n",
        " when model predict wrong prediction becuse it learn worng thing or make  too many assumption\n",
        " model bias:-when model is too simple and cant learn the pattern well\n",
        " data bias :- when data is unbalanced or unfair and model is train inthis data ex if data train that includes only men\n",
        " then its not work propely in women prediction\n",
        "\n",
        " 8. what is the importance of feature scaling in machine learning?\n",
        " -- feature scalling ensure that the feature are at a similar scale ,preventing certain feature to dominating the learning\n",
        " process and help the algoritm to train faster way\n",
        " 1.standarrization mean 0 and diviation =1\n",
        " 2.normalization value betweem 0 to 1\n",
        "\n",
        " 9. can you expalin regularization in machine learning and difference between L1 OR L2 regularization\n",
        " --To previent the model from over fitteng we use regularization (L1 and L2). regularization add penalty term to the model\n",
        " loss function.(L1- add panlty term in absolute value of weights and L2- add panalty term in square of the weights)\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Lasso,Ridge,LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metics import mean_squard_error\n",
        "from sklearn.datasets import make_regression\n",
        "x,y= make_regression(n_samples=100,n_features=5,random_sate=42)\n",
        "x_tarin,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "lr= LinearRegression()\n",
        "lr.fit(x_train,y_train)\n",
        "y_pred=lr.predict(x_test)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(x_train,y_train)\n",
        "y_pred_lasso=lasso.predict(x_test)\n",
        "ridge = Ridge(alpha=0.1)\n",
        "ridge.fit(x_train,y_train)\n",
        "y_pred_ridge=ridge.predict(x_test)\n",
        "print(mean_squared_error(y_test,y_pred))\n",
        "print(mean_squared_error(y_test,y_pred_lasso))\n",
        "print(mean_squared_error(y_test,y_pred_ridge))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "10.what is the purpose of the confusion matrix in classification task\n",
        "--The confusion matrix is a table that summarizes the performance of a classification model.\n",
        " It shows the number of true positives, true negatives, false positives, and false negatives. This information\n",
        " helps you understand how well your model is performing and where it's making mistakes.\n",
        ".name of all different metrics to evaluate the classification model's performance\n",
        "Accuracy\n",
        "Precision\n",
        "Recall (Sensitivity or True Positive Rate)\n",
        "F1-Score\n",
        "ROC Curve (Receiver Operating Characteristic Curve)\n",
        "AUC (Area Under the Curve)\n",
        "Log Loss (Cross-Entropy Loss)\n",
        "Specificity (True Negative Rate)\n",
        "Confusion Matrix\n",
        "Accuracy: This is the ratio of correctly predicted instances to the total number of instances.\n",
        " While simple, it can be misleading for imbalanced datasets.\n",
        " \"\"\"\n",
        " from sklearn.metrics import accuracy_score\n",
        "    # y_true and y_pred are the true and predicted labels respectively\n",
        "    # accuracy = accuracy_score(y_test, y_pred)\n",
        "\"\"\"\n",
        "Precision: This is the ratio of true positives to the total number of positive predictions\n",
        " (true positives + false positives). It measures the model's ability to avoid false positives.\n",
        "\"\"\"\n",
        "from sklearn.metrics import precision_score\n",
        "    # precision = precision_score(y_test, y_pred, average='binary') # for binary classification\n",
        "    # precision = precision_score(y_test, y_pred, average='macro') # for multi-class classification\n",
        "\"\"\"\n",
        "Recall (Sensitivity or True Positive Rate): This is the ratio of true positives to the total number\n",
        "of actual positives (true positives + false negatives). It measures the model's ability to find all\n",
        " positive instances.\n",
        "\"\"\"\n",
        "from sklearn.metrics import recall_score\n",
        "    # recall = recall_score(y_test, y_pred, average='binary') # for binary classification\n",
        "    # recall = recall_score(y_test, y_pred, average='macro') # for multi-class classification\n",
        "\"\"\"\n",
        "F1-Score: This is the harmonic mean of precision and recall. It provides a balance between\n",
        " precision and recall and is useful when you have an uneven class distribution.\n",
        "\"\"\"\n",
        "from sklearn.metrics import f1_score\n",
        "    # f1 = f1_score(y_test, y_pred, average='binary') # for binary classification\n",
        "    # f1 = f1_score(y_test, y_pred, average='macro') # for multi-class classification\n",
        "\"\"\"\n",
        "ROC Curve (Receiver Operating Characteristic Curve) and AUC (Area Under the Curve):\n",
        " The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate at\n",
        "  various threshold settings. AUC is the area under the ROC curve, and it represents the\n",
        "   model's ability to distinguish between positive and negative classes. A higher AUC\n",
        "   indicates better performance.\n",
        "\"\"\"\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "    # fpr, tpr, thresholds = roc_curve(y_test, y_scores) # y_scores are the predicted probabilities\n",
        "    # roc_auc = auc(fpr, tpr)\n",
        "\"\"\"\n",
        "Log Loss (Cross-Entropy Loss): This metric measures the performance of a classification\n",
        "model where the output is a probability value between 0 and 1. It penalizes false\n",
        "classifications heavily.\n",
        "\"\"\"\n",
        "from sklearn.metrics import log_loss\n",
        "    # logloss = log_loss(y_test, y_probabilities) # y_probabilities are the predicted probabilities"
      ],
      "metadata": {
        "id": "TJ15s-0dGecU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For regression, you would use different metrics to evaluate the model's performance, such as:\n",
        "\"\"\"\n",
        "Mean Squared Error (MSE): Measures the average squared difference between the predicted and actual values.\n",
        "\n",
        "Root Mean Squared Error (RMSE): The square root of the MSE, providing an error metric in the same units as the\n",
        "target variable.\n",
        "Mean Absolute Error (MAE): Measures the average absolute difference between the predicted and actual values.\n",
        "R-squared: Represents the proportion of the variance in the dependent variable that is predictable from the\n",
        " independent variables.\n",
        "\"\"\"\n",
        "from sklearn.metrics import mean_squared_error\n",
        "    import numpy as np\n",
        "# Example true and predicted values\n",
        "    y_test = np.array([3, -0.5, 2, 7])\n",
        "    y_pred = np.array([2.5, 0.0, 2, 8])\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "    import numpy as np\n",
        "\n",
        "    # Example true and predicted values\n",
        "    y_test = np.array([3, -0.5, 2, 7])\n",
        "    y_pred = np.array([2.5, 0.0, 2, 8])\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"Root Mean Squared Error: {rmse}\")\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "    import numpy as np\n",
        "\n",
        "    # Example true and predicted values\n",
        "    y_test = np.array([3, -0.5, 2, 7])\n",
        "    y_pred = np.array([2.5, 0.0, 2, 8])\n",
        "\n",
        "    # Calculate MAE\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(f\"Mean Absolute Error: {mae}\")\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "    import numpy as np\n",
        "\n",
        "    # Example true and predicted values\n",
        "    y_test = np.array([3, -0.5, 2, 7])\n",
        "    y_pred = np.array([2.5, 0.0, 2, 8])\n",
        "\n",
        "    # Calculate R-squared\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "njzhcXIeePfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4GF_TKgWftTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}