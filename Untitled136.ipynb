{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYqb3KkNnt2ttdWsXmiszT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishagrawa0503/daily-interview-qustion/blob/main/Untitled136.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "how do you handle multicollinearity in a dataset?\n",
        "-- multicollinerity happens when 2 or more fetures (independent variables) in your dataset are highly correlated - meanning\n",
        "provide  similar information\n",
        " handle multicollinerity -\n",
        " 1.drop one of the correlated feature.\n",
        " 2.combine correlated feature-PCA(principal component analysis) can also be used to combine correlated feature into\n",
        " uncorrelated components.also use reduce number of features (dimensionality) in a datset,\n",
        " ex- you have two feature one is height_in_cm , and other is height_in_inches these are highly correlated.pca can combine\n",
        " into one new feature.\n",
        " import pandas as pd\n",
        " from sklearn.preprocessing import StandardScaler\n",
        " from sklearn.decomposition import PCA\n",
        " df = pd.DataFrame({'feature1' : [2,4,6,8],\n",
        " 'feature2' : [12,14,16,18] , 'feature3' : [22,24,26,28]})\n",
        " pca = PCA(n_components=2)\n",
        " pca_data=pca.fit_transform(df)\n",
        "\n",
        " 3.regularization(ridge/lasso)--\n",
        " 4. use tree base structure like random forest,XGBoost are not sensitive to multicollinearity,because they dont rely on coefficients.\n",
        "\n",
        " #befor fixing it ,detect it using\n",
        "\n",
        " 1.correlation matrix-this table shows the correlation coeffcients between many variable.it help you quickly see how strong\n",
        " variable are related to each other which is use full for detecting multicollinearity.\n",
        " ex- import pandas as pd\n",
        " import numpy as np\n",
        " df = pd.read_csv(\"dataset.csv\")\n",
        " corr_matrix= df.corr()\n",
        " print (corr_matrix)\n",
        "  value range from (+1=possitive correlation,0=no correation,-1=perfect negative correlation\n",
        "\n",
        " 2.VIF(variance inflation factor)-is a metric used to quantify multicollenratiy in regression.it tells you how much the\n",
        " variance of a regression coefficients is increased due to correlations with other feature.\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4GF_TKgWftTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "explain the outlier ?\n",
        "outlier-an outlier ia a data point that different from other observation in dataset. it can arice from data entry error\n",
        ",measurement error ect\n",
        "common methods to handle outlier\n",
        "1.visualization - box plot,scatter plot, histogram\n",
        "2.z_score(standraization)-measures how far a data point is from the mean,in teram of standard division.\n",
        "datapoint with high absolute z-score are consider outlier\n",
        "3.IQR(interquartile range)- Q1= (25th percentile) 25 percent data below from this value\n",
        "  Q3= 75 percent data is below form, this value theN IQR = Q3 - Q1 then lower then = Q1-1.5*(IQR) higher then = Q3+1.5*(IQR)\n",
        " ideal for non normal distribution,\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "data = np.array([10,11,12,13,14,15,16])\n",
        "z_score= zscore(data)\n",
        "outlier = np.where(np.abs(z_score)>3)\n",
        "print (outlier[0])\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'age':[22,25,28,30,45]})\n",
        "Q1=df['age'].quantile(0.25)\n",
        "Q2=df['age'].quantile(0.75)\n",
        "IQR= Q3-Q1\n",
        "lower = Q1 - 1.5(IQR)\n",
        "higher = Q3 + 1.5(IQR)\n",
        "filtered_df=df[df['age']>= lower] & df[df['age']<= higher]\n",
        "print (filtered)\n"
      ],
      "metadata": {
        "id": "jmYqMovgS1QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "expalin the concept of ensemble learning ?\n",
        "-- ensemble learning is a machine learning technique that combines the predictions of multiple model to produce a\n",
        "storonge more accurate model( improve accueracy, overfitting,bias and variance trade off)\n",
        "types of ensemble methods :-\n",
        "bagging-reduce variance by averaging independent model . trains multiple model on random subset of the data\n",
        "boosting- reduce bias by learning from errors ex- gradient boosting,xgboosting,lightGBM\n",
        "   boosting focus on training each model sequentially, given more weight to misclassified data points\n",
        "\n",
        "stacking- combines different types of model (dicision tree ,svm, neral network)\n",
        "\"\"\"\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Train Random Forest (ensemble of decision trees)\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "cBYmSI-niLoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "how do you select the optimal number of cluster in a k-mean clustering algoritm?\n",
        "-1. elbow method in k-mean clustring-- data main number of cluster kitne honge usko decide krne ke liye elbow method ka\n",
        "upyog kiya jata hai.\n",
        "inertia-- wcss(within-cluster sum of squares) or inertia  ye k-mean ke andar ek function hota hai jisme cluster ka result\n",
        "hota hai.(wcss metrix used to evaluate how well the data point are clustered in k-mean). as you incress k,wcss decresses\n",
        "because cluster get smaller. but after some point ,adding more k (cluster) dosent significantly reduce WCSS -this is elbow\n",
        "point.\n",
        "step for elbow methpd- 1. run k-mean for different values of k (ex-1 to 10), 2.calculate the inertia for each k 3.plot k vs\n",
        "inertia, looking the point whaere curve is bends\n",
        "\"\"\"\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "x,_ = make_blobs(n_samples=100 ,clusters=5,random_state=42)\n",
        "wcss=[]\n",
        "for k in range(1,11):\n",
        "  kmeans=KMeans(n_clusters=k,random_state=42)\n",
        "  kmeans.fit(x)#x is data\n",
        "  wcss.append(kmeans.inertia_)\n",
        "\n",
        "\"\"\"\n",
        "2.silhouette score - measure how similar a point is to own cluster vs other cluster.score range from -1 to +1 higher=better\n",
        "definig cluster\n",
        "\"\"\"\n",
        "from sklearn.metrics import silhouette_score\n",
        "for k in range(2,11):\n",
        "  kmeans=KMeans(n_clusters=k,random_state=42)\n",
        "  labels= kmeans.fit_predict(x)\n",
        "  score=silhouette_score(x,label)\n",
        "  print ({k}:{score})"
      ],
      "metadata": {
        "id": "Dkr_vqAwR3R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " \"\"\"\n",
        " how do you handle a large volume of data that cannot fit into memory?\n",
        " --lagge vloumes of data can be handle using techinque such as data steaming or data chunking /batching\n",
        " 1. data chunking/batching-process the data in small chunks rether then loading it all at ones.\n",
        " 2.use efficient file formate- use binary formates that support partial loading loike parquet,feather,HDF5\n",
        " faster and smaller then csv\n",
        " \"\"\"\n",
        "import pandas as pd\n",
        "for chunk in pd.read_csv(\"large_dataset.csv\",chunksize=10000)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_parquet('data.parquet',columns=['col1','col2'])\n"
      ],
      "metadata": {
        "id": "Jp80QWUCbjnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "how do you handling a stuation where the data is too imbalanced?\n",
        "--a data set imbalance when:\n",
        "class A : 95% OF DATA\n",
        "class B : 5% OF DATA\n",
        "1.resampling techniques=\n",
        "from imblearn.over_sampling import SMOTE #(synthetic minority over sampling techinique)\n",
        "sm= SMOTE()\n",
        "x_resample,y_resample= sm.fit_resample(x,y)\n",
        "\n",
        "2. reduce the number of majority classes samples\n",
        "from imleran.under_sampling import RandomUnderSampler\n",
        "rus= RandomUnderSampler()\n",
        "x_resample,y_resample = rus.fit_resample(x,y)\n",
        "\n",
        "\n",
        "3.class weighting\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RnadomForestClassifier(class_weight='balanced')\n",
        "model.fit(x,y)\n",
        "\n",
        "4. anomaly detection- if the minority class is very rare,treat the task as anomaly detection\n",
        "from sklearn.ensemble import IsolationForest\n",
        "clf = IsolationForest()\n",
        "clf.fit(x)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Zv7732i8etjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RNN(Recurrent Neural Network)\n",
        "\"\"\"\n",
        "a recurrent neural network is a type of neural network specifically designed for sequntial or time series data.rnn is different\n",
        "from feedforward network because they have loops that allow them to remember previouse information in the sequence.\n",
        "take an input XT at time t, combine a new hiden state from the previous time t-1,produce new hidden state ht and\n",
        "an output yt.\n",
        "limitation of rnn - vanishing gradient problem makes harder to long term dependency,strugle with log sequence\n",
        "\"\"\"\n",
        "# RNN (recurrent neural network )\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN,Dense\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(50,input_shape(10,1)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam',loss='mse')\n",
        "model.summary()\n",
        "\n",
        "#LSTM and GRU improved the version of RNN\n",
        "\n",
        "\"\"\"\n",
        "LONG-SHORT-TERM-MEMORY- it was designed for overcome the short-term memory limitation of standard RNN .\n",
        "LSTM solve this problem by using gate==1. forget gate-descides what information to discard\n",
        "2. input gate - decides what new information to store\n",
        "3.ouput gate- decides what to output from the cell\n",
        "\"\"\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense\n",
        "model = Sequential()\n",
        "model.add(LSTM(50,input_shape=(10,1)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam',loss='mse')\n",
        "model.summary()\n",
        "\n",
        "\"\"\"\n",
        "GRU= GATED RCURRENT UNIT- it slove the limitation of rnn such as sort term memory and slow training .\n",
        "1.update gate- decides how much past information to keep\n",
        "2. reset gate- decides how much past info to forgt\n",
        "\"\"\"\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU,Dense\n",
        "model = Sequential([\n",
        "    GRU(64,input_shape(10,1),return_sequences=False),\n",
        "    Dense(1,activation='linear')\n",
        "                    ])\n",
        "model.compile(optimizer='adam'.loss='mse')\n",
        "model.summery()\n",
        "#gru  is faster then lstm\n",
        "ðŸ“š Use Cases\n",
        "\tâ€¢\tTime series forecasting (stock prices, weather)\n",
        "\tâ€¢\tNatural language processing (chatbots, translation)\n",
        "\tâ€¢\tAudio and speech recognition\n",
        "\tâ€¢\tAnomaly detection in sequences\n",
        "\n",
        "# FNN(feedforward neural network)\n",
        "\"\"\"\n",
        "simplest neural network ,data moves only one direction input to hiden to output without looping back. an fnn is used\n",
        "fro task where the input data is not sequential like email spam,house price predictionect\n",
        "\"\"\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        " model = Sequential()\n",
        " model.add(Dense(64,input_dim=10,activation='relu'))\n",
        " model.add(Dense(32,activation='relu'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "#CNN( CONVOLUTIONAL NEURAL NETWORK) :=\n",
        "\"\"\"\n",
        "CNN  are used to image data it is backbone of computer vision task,cnn are automatic extract feature like edge,shapes,taxture\n",
        "from data and learn patterns to make prediction.\n",
        "\"\"\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(64,64,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128,activation='relu'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "#CNN types of  convolutional layers\n",
        "\n",
        "1.standard(2D) - applies a set of kernal to input image to produce feature.most commonly use in image proccesing,3*3,5*5\n",
        "2.1D - used in sequential data or time series or text .\n",
        "3. 3D -used for videos or madicale imaginig like CT scane\n",
        "4. dilated(atrous)-expand the kernal by incressing space between kernal element without incressing parameter.\n",
        "ex 1 2 3              1   2   3\n",
        "   4 5 6              4   5   6\n",
        "   7 8 9              7   8   9\n"
      ],
      "metadata": {
        "id": "maRAuqrpltMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "what is the purpose of the WORD2vec algoritham in NLP?\n",
        "word2vec convert words into dense,low-dimensional vectos such that similar word have similar vector. this method to understand\n",
        "relationship between word based on context ,not just spalling\n",
        "\"\"\"\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentence,vector_size=10,window=2,min_count=1,sg=1)#sg=skip-gram"
      ],
      "metadata": {
        "id": "xAGGbSYViB9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "how do you handle a situation where there are too many feature compared to the number of observation?\n",
        "--PCA,T-SHINE\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NaWpmU3glvhL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}